### result of cifar10 original
2017-04-21 10:49:43.357676: step 438730, loss = 0.14 (1132.6 examples/sec; 0.113 sec/batch)
2017-04-21 10:49:44.449507: step 438740, loss = 0.13 (1172.3 examples/sec; 0.109 sec/batch)
2017-04-21 10:49:45.561496: step 438750, loss = 0.15 (1151.1 examples/sec; 0.111 sec/batch)
2017-04-21 10:49:46.661897: step 438760, loss = 0.11 (1163.2 examples/sec; 0.110 sec/batch)
2017-04-21 10:49:47.800181: step 438770, loss = 0.12 (1124.5 examples/sec; 0.114 sec/batch)
2017-04-21 10:49:48.902980: step 438780, loss = 0.14 (1160.7 examples/sec; 0.110 sec/batch)
2017-04-21 10:49:50.010745: step 438790, loss = 0.15 (1155.5 examples/sec; 0.111 sec/batch)
2017-04-21 10:49:51.238150: step 438800, loss = 0.13 (1042.9 examples/sec; 0.123 sec/batch)
2017-04-21 10:49:52.225483: step 438810, loss = 0.11 (1296.4 examples/sec; 0.099 sec/batch)
2017-04-21 10:49:53.340448: step 438820, loss = 0.12 (1148.0 examples/sec; 0.111 sec/batch)
2017-04-21 10:49:54.463862: step 438830, loss = 0.11 (1139.4 examples/sec; 0.112 sec/batch)
2017-04-21 10:49:55.602834: step 438840, loss = 0.11 (1123.8 examples/sec; 0.114 sec/batch)
2017-04-21 10:49:56.711436: step 438850, loss = 0.11 (1154.6 examples/sec; 0.111 sec/batch)
2017-04-21 10:49:57.837942: step 438860, loss = 0.11 (1136.3 examples/sec; 0.113 sec/batch)

2017-04-21 10:50:14.644544: precision @ 1 = 0.870

### result of cifar10 original
2017-04-21 12:38:48.075961: step 49770, loss = 0.74 (1071.1 examples/sec; 0.120 sec/batch)
2017-04-21 12:38:49.317086: step 49780, loss = 0.69 (1031.3 examples/sec; 0.124 sec/batch)
2017-04-21 12:38:50.601897: step 49790, loss = 0.72 (996.2 examples/sec; 0.128 sec/batch)
2017-04-21 12:38:51.972597: step 49800, loss = 0.71 (933.8 examples/sec; 0.137 sec/batch)
2017-04-21 12:38:53.045247: step 49810, loss = 0.70 (1193.3 examples/sec; 0.107 sec/batch)
2017-04-21 12:38:54.297514: step 49820, loss = 0.64 (1022.2 examples/sec; 0.125 sec/batch)
2017-04-21 12:38:55.531772: step 49830, loss = 0.66 (1037.1 examples/sec; 0.123 sec/batch)
2017-04-21 12:38:56.754786: step 49840, loss = 0.62 (1046.6 examples/sec; 0.122 sec/batch)
2017-04-21 12:38:58.034665: step 49850, loss = 0.74 (1000.1 examples/sec; 0.128 sec/batch)
2017-04-21 12:38:59.270995: step 49860, loss = 0.74 (1035.3 examples/sec; 0.124 sec/batch)
2017-04-21 12:39:00.484104: step 49870, loss = 0.99 (1055.1 examples/sec; 0.121 sec/batch)
2017-04-21 12:39:01.707335: step 49880, loss = 0.79 (1046.4 examples/sec; 0.122 sec/batch)
2017-04-21 12:39:02.939603: step 49890, loss = 0.81 (1038.7 examples/sec; 0.123 sec/batch)
2017-04-21 12:39:04.298296: step 49900, loss = 0.68 (942.1 examples/sec; 0.136 sec/batch)
2017-04-21 12:39:05.391504: step 49910, loss = 0.61 (1170.9 examples/sec; 0.109 sec/batch)
2017-04-21 12:39:06.622162: step 49920, loss = 0.80 (1040.1 examples/sec; 0.123 sec/batch)
2017-04-21 12:39:07.834040: step 49930, loss = 0.74 (1056.2 examples/sec; 0.121 sec/batch)
2017-04-21 12:39:09.065373: step 49940, loss = 0.80 (1039.5 examples/sec; 0.123 sec/batch)
2017-04-21 12:39:10.283589: step 49950, loss = 0.76 (1050.7 examples/sec; 0.122 sec/batch)
2017-04-21 12:39:11.515913: step 49960, loss = 0.75 (1038.7 examples/sec; 0.123 sec/batch)
2017-04-21 12:39:12.732593: step 49970, loss = 0.58 (1052.0 examples/sec; 0.122 sec/batch)
2017-04-21 12:39:13.975698: step 49980, loss = 0.59 (1029.7 examples/sec; 0.124 sec/batch)
2017-04-21 12:39:15.223052: step 49990, loss = 0.68 (1026.2 examples/sec; 0.125 sec/batch)

2017-04-21 12:57:15.110323: precision @ 1 = 0.854