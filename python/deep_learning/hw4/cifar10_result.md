### result of cifar10 original
2017-04-21 10:49:43.357676: step 438730, loss = 0.14 (1132.6 examples/sec; 0.113 sec/batch)
2017-04-21 10:49:44.449507: step 438740, loss = 0.13 (1172.3 examples/sec; 0.109 sec/batch)
2017-04-21 10:49:45.561496: step 438750, loss = 0.15 (1151.1 examples/sec; 0.111 sec/batch)
2017-04-21 10:49:46.661897: step 438760, loss = 0.11 (1163.2 examples/sec; 0.110 sec/batch)
2017-04-21 10:49:47.800181: step 438770, loss = 0.12 (1124.5 examples/sec; 0.114 sec/batch)
2017-04-21 10:49:48.902980: step 438780, loss = 0.14 (1160.7 examples/sec; 0.110 sec/batch)
2017-04-21 10:49:50.010745: step 438790, loss = 0.15 (1155.5 examples/sec; 0.111 sec/batch)
2017-04-21 10:49:51.238150: step 438800, loss = 0.13 (1042.9 examples/sec; 0.123 sec/batch)
2017-04-21 10:49:52.225483: step 438810, loss = 0.11 (1296.4 examples/sec; 0.099 sec/batch)
2017-04-21 10:49:53.340448: step 438820, loss = 0.12 (1148.0 examples/sec; 0.111 sec/batch)
2017-04-21 10:49:54.463862: step 438830, loss = 0.11 (1139.4 examples/sec; 0.112 sec/batch)
2017-04-21 10:49:55.602834: step 438840, loss = 0.11 (1123.8 examples/sec; 0.114 sec/batch)
2017-04-21 10:49:56.711436: step 438850, loss = 0.11 (1154.6 examples/sec; 0.111 sec/batch)
2017-04-21 10:49:57.837942: step 438860, loss = 0.11 (1136.3 examples/sec; 0.113 sec/batch)

2017-04-21 10:50:14.644544: precision @ 1 = 0.870

### result of cifar10 original
2017-04-21 12:38:48.075961: step 49770, loss = 0.74 (1071.1 examples/sec; 0.120 sec/batch)
2017-04-21 12:38:49.317086: step 49780, loss = 0.69 (1031.3 examples/sec; 0.124 sec/batch)
2017-04-21 12:38:50.601897: step 49790, loss = 0.72 (996.2 examples/sec; 0.128 sec/batch)
2017-04-21 12:38:51.972597: step 49800, loss = 0.71 (933.8 examples/sec; 0.137 sec/batch)
2017-04-21 12:38:53.045247: step 49810, loss = 0.70 (1193.3 examples/sec; 0.107 sec/batch)
2017-04-21 12:38:54.297514: step 49820, loss = 0.64 (1022.2 examples/sec; 0.125 sec/batch)
2017-04-21 12:38:55.531772: step 49830, loss = 0.66 (1037.1 examples/sec; 0.123 sec/batch)
2017-04-21 12:38:56.754786: step 49840, loss = 0.62 (1046.6 examples/sec; 0.122 sec/batch)
2017-04-21 12:38:58.034665: step 49850, loss = 0.74 (1000.1 examples/sec; 0.128 sec/batch)
2017-04-21 12:38:59.270995: step 49860, loss = 0.74 (1035.3 examples/sec; 0.124 sec/batch)
2017-04-21 12:39:00.484104: step 49870, loss = 0.99 (1055.1 examples/sec; 0.121 sec/batch)
2017-04-21 12:39:01.707335: step 49880, loss = 0.79 (1046.4 examples/sec; 0.122 sec/batch)
2017-04-21 12:39:02.939603: step 49890, loss = 0.81 (1038.7 examples/sec; 0.123 sec/batch)
2017-04-21 12:39:04.298296: step 49900, loss = 0.68 (942.1 examples/sec; 0.136 sec/batch)
2017-04-21 12:39:05.391504: step 49910, loss = 0.61 (1170.9 examples/sec; 0.109 sec/batch)
2017-04-21 12:39:06.622162: step 49920, loss = 0.80 (1040.1 examples/sec; 0.123 sec/batch)
2017-04-21 12:39:07.834040: step 49930, loss = 0.74 (1056.2 examples/sec; 0.121 sec/batch)
2017-04-21 12:39:09.065373: step 49940, loss = 0.80 (1039.5 examples/sec; 0.123 sec/batch)
2017-04-21 12:39:10.283589: step 49950, loss = 0.76 (1050.7 examples/sec; 0.122 sec/batch)
2017-04-21 12:39:11.515913: step 49960, loss = 0.75 (1038.7 examples/sec; 0.123 sec/batch)
2017-04-21 12:39:12.732593: step 49970, loss = 0.58 (1052.0 examples/sec; 0.122 sec/batch)
2017-04-21 12:39:13.975698: step 49980, loss = 0.59 (1029.7 examples/sec; 0.124 sec/batch)
2017-04-21 12:39:15.223052: step 49990, loss = 0.68 (1026.2 examples/sec; 0.125 sec/batch)

2017-04-21 12:57:15.110323: precision @ 1 = 0.854

### result of cifar10_3conv_32_32_64
2017-04-21 20:22:55.373623: step 49740, loss = 0.50 (1308.0 examples/sec; 0.098 sec/batch)
2017-04-21 20:22:56.324914: step 49750, loss = 0.89 (1345.5 examples/sec; 0.095 sec/batch)
2017-04-21 20:22:57.276154: step 49760, loss = 0.64 (1345.6 examples/sec; 0.095 sec/batch)
2017-04-21 20:22:58.209064: step 49770, loss = 0.66 (1372.0 examples/sec; 0.093 sec/batch)
2017-04-21 20:22:59.162844: step 49780, loss = 0.62 (1342.0 examples/sec; 0.095 sec/batch)
2017-04-21 20:23:00.104410: step 49790, loss = 0.55 (1359.4 examples/sec; 0.094 sec/batch)
2017-04-21 20:23:01.097419: step 49800, loss = 0.62 (1289.0 examples/sec; 0.099 sec/batch)
2017-04-21 20:23:02.009282: step 49810, loss = 0.73 (1403.7 examples/sec; 0.091 sec/batch)
2017-04-21 20:23:02.965910: step 49820, loss = 0.63 (1338.0 examples/sec; 0.096 sec/batch)
2017-04-21 20:23:03.908657: step 49830, loss = 0.57 (1357.7 examples/sec; 0.094 sec/batch)
2017-04-21 20:23:04.847948: step 49840, loss = 0.74 (1362.7 examples/sec; 0.094 sec/batch)
2017-04-21 20:23:05.783279: step 49850, loss = 0.65 (1368.5 examples/sec; 0.094 sec/batch)
2017-04-21 20:23:06.733483: step 49860, loss = 0.64 (1347.1 examples/sec; 0.095 sec/batch)
2017-04-21 20:23:07.683235: step 49870, loss = 0.52 (1347.7 examples/sec; 0.095 sec/batch)
2017-04-21 20:23:08.625194: step 49880, loss = 0.78 (1358.9 examples/sec; 0.094 sec/batch)
2017-04-21 20:23:09.567363: step 49890, loss = 0.71 (1358.6 examples/sec; 0.094 sec/batch)
2017-04-21 20:23:10.543959: step 49900, loss = 0.53 (1310.7 examples/sec; 0.098 sec/batch)
2017-04-21 20:23:11.456714: step 49910, loss = 0.64 (1402.3 examples/sec; 0.091 sec/batch)
2017-04-21 20:23:12.385711: step 49920, loss = 0.65 (1377.8 examples/sec; 0.093 sec/batch)
2017-04-21 20:23:13.338372: step 49930, loss = 0.78 (1343.6 examples/sec; 0.095 sec/batch)
2017-04-21 20:23:14.274232: step 49940, loss = 0.68 (1367.7 examples/sec; 0.094 sec/batch)
2017-04-21 20:23:15.227485: step 49950, loss = 0.77 (1342.8 examples/sec; 0.095 sec/batch)
2017-04-21 20:23:16.157790: step 49960, loss = 0.50 (1375.9 examples/sec; 0.093 sec/batch)
2017-04-21 20:23:17.112762: step 49970, loss = 0.63 (1340.3 examples/sec; 0.095 sec/batch)
2017-04-21 20:23:18.056657: step 49980, loss = 0.62 (1356.1 examples/sec; 0.094 sec/batch)
2017-04-21 20:23:19.001197: step 49990, loss = 0.66 (1355.1 examples/sec; 0.094 sec/batch)

2017-04-21 21:06:45.512474: precision @ 1 = 0.834

### result of cifar10_3conv_32_32_64
2017-04-22 02:34:42.860102: step 199730, loss = 0.28 (1299.4 examples/sec; 0.099 sec/batch)
2017-04-22 02:34:43.849731: step 199740, loss = 0.36 (1293.4 examples/sec; 0.099 sec/batch)
2017-04-22 02:34:44.800837: step 199750, loss = 0.27 (1345.8 examples/sec; 0.095 sec/batch)
2017-04-22 02:34:45.792893: step 199760, loss = 0.33 (1290.3 examples/sec; 0.099 sec/batch)
2017-04-22 02:34:46.796066: step 199770, loss = 0.31 (1276.0 examples/sec; 0.100 sec/batch)
2017-04-22 02:34:47.784883: step 199780, loss = 0.27 (1294.5 examples/sec; 0.099 sec/batch)
2017-04-22 02:34:48.760055: step 199790, loss = 0.30 (1312.6 examples/sec; 0.098 sec/batch)
2017-04-22 02:34:49.786863: step 199800, loss = 0.37 (1246.6 examples/sec; 0.103 sec/batch)
2017-04-22 02:34:50.696688: step 199810, loss = 0.29 (1406.9 examples/sec; 0.091 sec/batch)
2017-04-22 02:34:51.653025: step 199820, loss = 0.36 (1338.5 examples/sec; 0.096 sec/batch)
2017-04-22 02:34:52.608134: step 199830, loss = 0.54 (1340.2 examples/sec; 0.096 sec/batch)
2017-04-22 02:34:53.568213: step 199840, loss = 0.26 (1333.2 examples/sec; 0.096 sec/batch)
2017-04-22 02:34:54.537665: step 199850, loss = 0.38 (1320.3 examples/sec; 0.097 sec/batch)
2017-04-22 02:34:55.494972: step 199860, loss = 0.29 (1337.1 examples/sec; 0.096 sec/batch)
2017-04-22 02:34:56.448625: step 199870, loss = 0.43 (1342.2 examples/sec; 0.095 sec/batch)
2017-04-22 02:34:57.421967: step 199880, loss = 0.33 (1315.1 examples/sec; 0.097 sec/batch)
2017-04-22 02:34:58.431000: step 199890, loss = 0.33 (1268.5 examples/sec; 0.101 sec/batch)
2017-04-22 02:34:59.575757: step 199900, loss = 0.28 (1118.1 examples/sec; 0.114 sec/batch)
2017-04-22 02:35:00.616149: step 199910, loss = 0.31 (1230.3 examples/sec; 0.104 sec/batch)
2017-04-22 02:35:01.636641: step 199920, loss = 0.36 (1254.3 examples/sec; 0.102 sec/batch)
2017-04-22 02:35:02.639740: step 199930, loss = 0.42 (1276.1 examples/sec; 0.100 sec/batch)
2017-04-22 02:35:03.643692: step 199940, loss = 0.35 (1275.0 examples/sec; 0.100 sec/batch)
2017-04-22 02:35:04.625261: step 199950, loss = 0.32 (1304.0 examples/sec; 0.098 sec/batch)
2017-04-22 02:35:05.628415: step 199960, loss = 0.40 (1276.0 examples/sec; 0.100 sec/batch)
2017-04-22 02:35:06.630871: step 199970, loss = 0.27 (1276.9 examples/sec; 0.100 sec/batch)
2017-04-22 02:35:07.636670: step 199980, loss = 0.30 (1272.6 examples/sec; 0.101 sec/batch)
2017-04-22 02:35:08.615497: step 199990, loss = 0.27 (1307.7 examples/sec; 0.098 sec/batch)

2017-04-22 11:42:12.524210: precision @ 1 = 0.853
