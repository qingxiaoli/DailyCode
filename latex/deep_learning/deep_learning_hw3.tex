%!TEX program = xelatex
\documentclass[a4paper, UTF8]{ctexrep}
\usepackage{ctex}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{bm}
\usepackage{subfigure}
\usepackage{float}
\usepackage{array}
\usepackage{makecell}

\renewcommand\thesection{\arabic{section}}

\begin{document}
    \begin{titlepage}
        \centering
        \vspace{6cm}
        \LARGE{\textbf{Deep Learning Homework 3}}\\
        \vspace{4cm}
        \includegraphics[width=0.8\textwidth]{deepLearning.png}\\
        \vspace{4cm}
        \normalsize{安捷 1601210097}\\
        \normalsize{\today}
    \end{titlepage}
        \section{算法实现简介}
            在这次作业中，我基于上一次作业实现的CNN模型，给CNN中添加了batch normalization层；同时，为了使得算法的代码更为清楚，我对上一次实现的代码进行了重构。
        \section{算法实现的函数功能简介}
            \begin{table}[htbp!]
                \centering
                \begin{tabular}{cc}
                    \hline
                    函数名称 & 功能 \\
                    \hline
                    conv\_2d & 卷积层线性单元 \\
                    max\_pool & 最大池化层 \\
                    evaluation & 计算准确率函数 \\
                    batch\_norm & batch norm层 \\
                    conv\_net & 卷积网络演化层 \\
                    \hline
                \end{tabular}
                \caption{算法实现的函数功能表}
            \end{table}
        \section{数值实验结果} % (fold)
        \label{sec:数值实验结果}
          我测试了增加batch normalization层的cnn的训练情况，我发现，在添加batch normalization之后，在使用Adam算法，学习率设置为0.01的情况下，经过10000次迭代，算法在训练集达到了98.7\%的准确率，性能远远不及不使用batch normorlization的情况，对比如下图：
            \begin{table}[htbp!]
                \centering
                \begin{tabular}{cccc}
                    \hline
                    优化算法 & 学习率 & 迭代次数 & 测试集准确率 \\
                    \hline
                    Adam & 0.001 & 2500 & 99\% \\
                    Adam & 0.01 & 10000 & 98.7\% \\
                    \hline
                \end{tabular}
                \caption{算法性能对比}
            \end{table}
            上表的结果令我比较惊讶，并且我发现，由于添加bn层之后每一次训练的计算量加大，每一次训练的时间变长。在这个数值试验中，bn层主要起到了学习率去单位化的作用，增加的训练时的稳定性，即学习率可以设置为原来的10倍算法依然可以收敛，但是没有起到加速训练的作用。
        % section 数值实验结果 (end)

    \section{代码运行环境及测试平台信息}
      \begin{table}[htbp!]
        \centering
        \begin{tabular}{l}
          \hline
          Python Version: 3.6.0 \\
          Tensorflow Version: tensorflow-gpu-1.0.1 \\
          CUDA Version: 8.0 \\
          OS: Arch Linux \\
          Kernel: x86\_64 Linux 4.10.4-1-ARCH \\
          CPU: Intel Core i7-6700K @ 8x 4.2GHz \\
          GPU: GeForce GTX 1060 6GB \\
          RAM: 16003MiB \\
          \hline
        \end{tabular}
        \caption{代码运行环境及测试环境表}
      \end{table}
      在没有NVIDIA\ GPU及CUDA支持的环境下代码依然可以运行，只是速度较慢
    \section{总结}
      通过这次作业，我学习了tensorflow实现cnn with bn的基本方法，了解了bn的原理和提高算法训练稳定性的作用，但是由于我在数值试验中没有观察到bn提高了算法训练的速度，因此对于bn可以提高网络的训练性能依然存在疑问。
\end{document}